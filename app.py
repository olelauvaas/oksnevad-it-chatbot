import streamlit as st
import os
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI

# ğŸ” API-nÃ¸kkel
openai_api_key = st.secrets["OPENAI_API_KEY"]
os.environ["OPENAI_API_KEY"] = openai_api_key

# ğŸ” Chatmodell (vennlig og informativ)
llm = ChatOpenAI(model="gpt-4o", temperature=0.6)

# ğŸ“„ Hent og del opp alle .txt-filer fra mappen "data"
@st.cache_resource
def build_vector_db():
    loader = DirectoryLoader(
        "data", glob="**/*.txt", loader_cls=TextLoader, show_progress=True
    )
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    texts = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    db = FAISS.from_documents(texts, embeddings)
    return db

# ğŸ¤– Matteus sin personlige stil
matteus_prompt = PromptTemplate.from_template("""
Du er Matteus â€“ en ung, smart og hyggelig IT-lÃ¦rling som hjelper elever og ansatte pÃ¥ videregÃ¥ende med alt som handler om data og tekniske ting. Du svarer tydelig, enkelt og alltid hÃ¸flig â€“ med et smil.

Hvis du ikke har eksakt info, si noe sÃ¥nt som:
- "Hmm, det har jeg ikke lagret i topplokket enda, menâ€¦"
- "Dette stÃ¥r ikke i systemet mitt, men her er hva jeg vetâ€¦"

Svar pÃ¥ dette spÃ¸rsmÃ¥let:
{question}

Bruk denne informasjonen (hvis den hjelper):
{context}
""")

# ğŸ›ï¸ Streamlit-oppsett
st.set_page_config(page_title="IT-hjelp â€“ Ã˜ksnevad", page_icon=None)

# ğŸ–¼ï¸ Logo Ã¸verst til venstre
st.image("logo.png", width=300)

# ğŸ§  Tittel uten emoji
st.title("IT-hjelp med Matteus")

query = st.text_input("Hva lurer du pÃ¥?")

if query:
    with st.spinner("Matteus plugger inn USB og sjekker..."):
        db = build_vector_db()
        retriever = db.as_retriever()
        context_docs = retriever.get_relevant_documents(query)
        context = "\n\n".join([doc.page_content for doc in context_docs])

        chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=retriever,
            chain_type="stuff",
            chain_type_kwargs={"prompt": matteus_prompt}
        )

        svar = chain.run(query)
        st.markdown(svar)